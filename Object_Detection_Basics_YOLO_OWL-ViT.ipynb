{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection  - YOLO & OWL-ViT\n",
    "This tutorial demonstrates how to use YOLO (You Only Look Once) from the [Ultralytics](https://github.com/ultralytics/yolov5) library for object detection. It includes steps for:\n",
    "\n",
    "- Running object detection inference on images/videos\n",
    "- Fine-tuning YOLO for custom datasets\n",
    "- Comparing YOLO with OWl-VIT for zero-shot learning.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Perform Object Detection Inference\n",
    "First thing We'll use YOLOv8 from Ultralyics for object detection on a sample image.\n",
    "We aim to utilize the pre-trained YOLOv8 model to detect objects in a sample image. This involves loading the model, providing an image for input, and interpreting the model's predictions.\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Inference**: The process of using a trained model to make predictions on new data.\n",
    "- **YOLOv8**: A state-of-the-art version of the YOLO (You Only Look Once) architecture, known for its speed and accuracy in object detection tasks.\n",
    "\n",
    "**Steps:**\n",
    "1. Load the YOLOv8 model using the Ultralytics library.\n",
    "2. Perform inference on a sample image to detect objects.\n",
    "3. Visualize the results, including bounding boxes and class labels.\n",
    "\n",
    "**Support Material:**\n",
    "- https://docs.ultralytics.com/models/yolov8/\n",
    "- https://docs.ultralytics.com/tasks/detect/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import YOLO and load a pre-trained model\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "\n",
    "# Load the YOLOv8 pre-trained model\n",
    "model = YOLO('yolov8n.pt')  # nano model for quick inference\n",
    "\n",
    "# Run inference on a sample image\n",
    "\n",
    "results = model('images/street_scene.jpg', save = False)  # Displays image with detections\n",
    "\n",
    "for result in results:\n",
    "    print(result.boxes)  # Boxes object for bounding box outputs\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fine-Tuning YOLO on Custom Dataset\n",
    "Fine-tuning YOLO requires a dataset formatted in the YOLO format. We'll use a small public dataset for demonstration.\n",
    "We will adapt the pre-trained YOLO model to a custom dataset. This process, known as fine-tuning, enables YOLO to specialize in detecting specific objects not included in its original training.\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Fine-tuning**: Adapting a pre-trained model to new data by continuing the training process.\n",
    "- **Custom Dataset**: A dataset that contains specific objects relevant to a new application, different from those YOLO was trained on (e.g. https://docs.ultralytics.com/datasets/detect/signature/.) Does it work? yes, no? why not? what can you do?\n",
    "\n",
    "**Steps:**\n",
    "1. Prepare the custom dataset by organizing images and labels in the required format.\n",
    "2. Configure the YOLO training pipeline.\n",
    "3. Train the model and evaluate its performance.\n",
    "\n",
    "**Support Material:** \n",
    "- https://docs.ultralytics.com/modes/train/\n",
    "- https://docs.ultralytics.com/modes/val/\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a sample dataset (e.g., Signature)\n",
    "!wget -q https://github.com/ultralytics/assets/releases/download/v0.0.0/signature.zip\n",
    "!unzip -q signature.zip -d ./datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train YOLO on the dataset\n",
    "results = model.train(data='./datasets/signature.yaml', epochs=10, imgsz=640, batch=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"runs/detect/train2/weights/best.pt\")  # load a custom model, check the path depending on your output before!!\n",
    "\n",
    "# Predict with the model\n",
    "results = model.predict(\"images/example_signature.jpg\", conf=0.75) #check params if you need to improve detection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Zero-Shot Learning with OWL-ViT\n",
    "Switch to `OWL-ViT` to see how it performs with zero-shot learning capabilities. Zero-shot means detecting objects without prior specific training.\n",
    "\n",
    "OWL-ViT (Open Vocabulary Learning with Vision Transformers) is a cutting-edge model designed for open vocabulary object detection. Unlike traditional models, OWL-ViT combines vision transformers with text embeddings, enabling it to:\\n\\n\n",
    "- Understand textual descriptions of objects, even if it hasn't seen them during training.\n",
    "- Detect and classify objects based on descriptive input, making it suitable for diverse applications.\n",
    "- Perform zero-shot learning by generalizing to new object classes without additional training.\\n\\n\"\n",
    "\n",
    "**Steps in Using OWL-ViT:**\n",
    "1. Model Initialization**: Set up the OWL-ViT model.\n",
    "2. Text Input for Object Descriptions: Provide descriptive prompts (e.g., 'a red car' or 'a black cat to guide detection.\n",
    "3. Inference and Visualization: Process an image or video, detect objects based on text descriptions and visualize results.\\n\\n\"\n",
    "\n",
    "OWL-ViT excels in scenarios where predefined object classes are insufficient, such as detecting rare or domain-specific objects.\n",
    "\n",
    "**Support Material**:\n",
    "- https://huggingface.co/docs/transformers/en/model_doc/owlvit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as pe\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "from transformers import OwlViTProcessor, OwlViTForObjectDetection\n",
    "\n",
    "image = Image.open(\"images/street_scene.jpg\")\n",
    "\n",
    "processor = OwlViTProcessor.from_pretrained(\"google/owlvit-base-patch32\")\n",
    "model = OwlViTForObjectDetection.from_pretrained(\"google/owlvit-base-patch32\")\n",
    "\n",
    "\n",
    "text_labels = [[\"a person on the floor\", \"a church \"]]\n",
    "\n",
    "inputs = processor(text=text_labels, images=image, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Target image sizes (height, width) to rescale box predictions [batch_size, 2]\n",
    "target_sizes = torch.tensor([(image.height, image.width)])\n",
    "\n",
    "# Convert outputs (bounding boxes and class logits) to Pascal VOC format (xmin, ymin, xmax, ymax)\n",
    "results = processor.post_process_grounded_object_detection(\n",
    "    outputs=outputs, target_sizes=target_sizes, threshold=0.1, text_labels=text_labels\n",
    ")\n",
    "# Retrieve predictions for the first image for the corresponding text queries\n",
    "result = results[0]\n",
    "boxes, scores, text_labels = result[\"boxes\"], result[\"scores\"], result[\"text_labels\"]\n",
    "\n",
    "for box, score, text_label in zip(boxes, scores, text_labels):\n",
    "    box = [round(i, 2) for i in box.tolist()]\n",
    "    print(f\"Detected {text_label} with confidence {round(score.item(), 3)} at location {box}\")\n",
    "\n",
    "\n",
    "def show_box(box, ax):\n",
    "    print(box)\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(\n",
    "        plt.Rectangle((x0, y0), w, h, edgecolor=\"green\", facecolor=(0, 0, 0, 0), lw=2)\n",
    "    )\n",
    "\n",
    "\n",
    "def show_boxes_and_labels_on_image(raw_image, boxes, labels, scores):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(raw_image)\n",
    "    for i, box in enumerate(boxes):\n",
    "        box = [round(i, 2) for i in box.tolist()]\n",
    "        show_box(box, plt.gca())\n",
    "        plt.text(\n",
    "            x=box[0],\n",
    "            y=box[1] - 12,\n",
    "            s=f\"{labels[i]}: {scores[i]:,.4f}\",\n",
    "            c=\"beige\",\n",
    "            path_effects=[pe.withStroke(linewidth=4, foreground=\"darkgreen\")],\n",
    "        )\n",
    "    plt.axis(\"on\")\n",
    "    plt.show()\n",
    "    #lt.savefig(\"streetscene_with_detections.jpg\")\n",
    "\n",
    "\n",
    "\n",
    "# Show the image with the bounding boxes\n",
    "show_boxes_and_labels_on_image(\n",
    "    image,\n",
    "    boxes,\n",
    "    text_labels,\n",
    "    scores\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
